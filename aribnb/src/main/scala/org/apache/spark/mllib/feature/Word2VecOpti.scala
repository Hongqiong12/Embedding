/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.mllib.feature

import java.lang.{Iterable => JavaIterable}
import scala.collection.JavaConverters._
import scala.collection.{immutable, mutable}
import com.github.fommil.netlib.BLAS.{getInstance => blas}
import org.json4s.DefaultFormats
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods._
import org.apache.spark.SparkContext
import org.apache.spark.annotation.Since
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.internal.Logging
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.util.{Loader, Saveable}
import org.apache.spark.rdd._
import org.apache.spark.sql.SparkSession
import org.apache.spark.util.BoundedPriorityQueue
import org.apache.spark.util.Utils
import org.apache.spark.util.random.XORShiftRandom

import scala.collection.mutable.ArrayBuffer



/**
 * Word2VecOpti creates vector representation of words in a text corpus.
 * The algorithm first constructs a vocabulary from the corpus
 * and then learns vector representation of words in the vocabulary.
 * The vector representation can be used as features in
 * natural language processing and machine learning algorithms.
 *
 * We used skip-gram model in our implementation and hierarchical softmax
 * method to train the model. The variable names in the implementation
 * matches the original C implementation.
 *
 * For original C implementation, see https://code.google.com/p/Word2VecOpti/
 * For research papers, see
 * Efficient Estimation of Word Representations in Vector Space
 * and
 * Distributed Representations of Words and Phrases and their Compositionality.
 */
@Since("1.1.0")
class Word2VecOpti extends Serializable with Logging {


  private var vectorSize = 100
  private var learningRate = 0.025
  private var numPartitions = 1
  private var numIterations = 1
  private var seed = Utils.random.nextLong()
  private var minCount = 5
  private var maxSentenceLength = 1000
  private var EOC=""

  def setEoc(EOC: String): this.type = {
    // å¯ä»¥å…è®¸è¾“å…¥çš„åˆå§‹åŒ–çš„å‚æ•°æ˜¯ç©ºçš„ï¼Œæ­¤æ—¶å°±é€‰æ‹©é»˜è®¤çš„åˆå§‹åŒ–çš„æ–¹å¼
    println(EOC.isEmpty)
    this.EOC = EOC
    this
  }

  /**
   * Sets the maximum length (in words) of each sentence in the input data.
   * Any sentence longer than this threshold will be divided into chunks of
   * up to `maxSentenceLength` size (default: 1000)
   */
  @Since("2.0.0")
  def setMaxSentenceLength(maxSentenceLength: Int): this.type = {
    require(maxSentenceLength > 0,
      s"Maximum length of sentences must be positive but got ${maxSentenceLength}")
    this.maxSentenceLength = maxSentenceLength
    this
  }

  /**
   * Sets vector size (default: 100).
   */
  @Since("1.1.0")
  def setVectorSize(vectorSize: Int): this.type = {
    require(vectorSize > 0,
      s"vector size must be positive but got ${vectorSize}")
    this.vectorSize = vectorSize
    this
  }

  /**
   * Sets initial learning rate (default: 0.025).
   */
  @Since("1.1.0")
  def setLearningRate(learningRate: Double): this.type = {
    require(learningRate > 0,
      s"Initial learning rate must be positive but got ${learningRate}")
    this.learningRate = learningRate
    this
  }

  /**
   * Sets number of partitions (default: 1). Use a small number for accuracy.
   */
  @Since("1.1.0")
  def setNumPartitions(numPartitions: Int): this.type = {
    require(numPartitions > 0,
      s"Number of partitions must be positive but got ${numPartitions}")
    this.numPartitions = numPartitions
    this
  }

  /**
   * Sets number of iterations (default: 1), which should be smaller than or equal to number of
   * partitions.
   */
  @Since("1.1.0")
  def setNumIterations(numIterations: Int): this.type = {
    require(numIterations >= 0,
      s"Number of iterations must be nonnegative but got ${numIterations}")
    this.numIterations = numIterations
    this
  }

  /**
   * Sets random seed (default: a random long integer).
   */
  @Since("1.1.0")
  def setSeed(seed: Long): this.type = {
    this.seed = seed
    this
  }

  /**
   * Sets the window of words (default: 5)
   */
  @Since("1.6.0")
  def setWindowSize(window: Int): this.type = {
    require(window > 0,
      s"Window of words must be positive but got ${window}")
    this.window = window
    this
  }

  /**
   * Sets minCount, the minimum number of times a token must appear to be included in the Word2VecOpti
   * model's vocabulary (default: 5).
   */
  @Since("1.3.0")
  def setMinCount(minCount: Int): this.type = {
    require(minCount >= 0,
      s"Minimum number of times must be nonnegative but got ${minCount}")
    this.minCount = minCount
    this
  }

  private val EXP_TABLE_SIZE = 1000
  private val MAX_EXP = 6
  private val MAX_CODE_LENGTH = 40

  /** context words from [-window, window] */
  private var window = 5

  private var trainWordsCount = 0L //è®­ç»ƒè¯çš„ä¸ªæ•°ï¼Œä¸å»é‡ç»Ÿè®¡ï¼Œæ¯”å¦‚å•è¯Aå‡ºç°10æ¬¡ï¼Œåˆ™æ€»è®¡è¯é¢‘çš„æ—¶å€™å°±æœª10ï¼Œå’ŒvocabSizeçš„åŒºåˆ«æ˜¯ä¸€ä¸ªæœªå»é‡ç»Ÿè®¡ä¸€ä¸ªå»é‡ç»Ÿè®¡
  private var vocabSize = 0 // è¯çš„ä¸ªæ•°
  @transient private var vocab: Array[VocabWord] = null
  @transient private var vocabHash = mutable.HashMap.empty[String, Int]

  private def learnVocab[S <: Iterable[String]](dataset: RDD[S]): Unit = {
    //TODO STEP1ï¼šå°†-1çš„ID å»é™¤ï¼Œå› ä¸º-1ä»£è¡¨çš„æ˜¯å°¾éƒ¨çš„å•†å“æ˜¯ä¸‹å•çš„å•†å“
    val words = dataset.flatMap(x => x)
      .filter(x=>x!=EOC)//æ„é€ è¯å…¸çš„æ—¶å€™ä¸å°†-1åŠ å…¥ï¼Œè¯¥å­—ç¬¦å½“åšç»ˆæ­¢å­—ç¬¦
      .filter(x=>x!=" ")//ä¹±ç ç¬¦å·å‰”é™¤
      .filter(x=>x.nonEmpty)//æ„é€ è¯å…¸çš„æ—¶å€™ä¸å°†-1åŠ å…¥ï¼Œè¯¥å­—ç¬¦å½“åšç»ˆæ­¢å­—ç¬¦

    //ç»Ÿè®¡æ¯ä¸ªè¯çš„è¯é¢‘ï¼Œå¹¶ä¸”æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—
    vocab = words.map(w => (w, 1))
      .reduceByKey(_ + _)
      .filter(_._2 >= minCount)
      .map(x => VocabWord(
        x._1,
        x._2,
        new Array[Int](MAX_CODE_LENGTH),
        new Array[Int](MAX_CODE_LENGTH),
        0))
      .collect()
      .sortBy(_.cn)(Ordering[Long].reverse)

    vocabSize = vocab.length
    require(vocabSize > 0, "The vocabulary size should be > 0. You may need to check " +
      "the setting of minCount, which could be large enough to remove all your words in sentences.")

    // æ„é€ HASHè¡¨ï¼Œä¹Ÿå°±æ˜¯å•è¯å’Œå…¶ä¸‹æ ‡çš„å¯¹åº”å…³ç³»ã€‚æ¯”å¦‚å•è¯Açš„è¯é¢‘æœ€é«˜ï¼Œåˆ™å…¶å¯¹åº”çš„ç´¢å¼•ä¸º0ï¼Œä¾æ¬¡ç±»æ¨ï¼ŒæŒ‰è¯é¢‘ä»é«˜åˆ°ä½æ„å»ºç´¢å¼•
    var a = 0
    while (a < vocabSize) {
      vocabHash += vocab(a).word -> a
      trainWordsCount += vocab(a).cn
      a += 1
    }
    logInfo(s"vocabSize = $vocabSize, trainWordsCount = $trainWordsCount")
  }

  private def createExpTable(): Array[Float] = {
    val expTable = new Array[Float](EXP_TABLE_SIZE)
    var i = 0
    while (i < EXP_TABLE_SIZE) {
      val tmp = math.exp((2.0 * i / EXP_TABLE_SIZE - 1.0) * MAX_EXP)
      expTable(i) = (tmp / (tmp + 1.0)).toFloat
      i += 1
    }
    expTable
  }

  // æ„é€ HUFFMANæ ‘
  private def createBinaryTree(): Unit = {
    val count = new Array[Long](vocabSize * 2 + 1)
    val binary = new Array[Int](vocabSize * 2 + 1)
    val parentNode = new Array[Int](vocabSize * 2 + 1)
    val code = new Array[Int](MAX_CODE_LENGTH)
    val point = new Array[Int](MAX_CODE_LENGTH)
    var a = 0
    while (a < vocabSize) {
      count(a) = vocab(a).cn
      a += 1
    }
    while (a < 2 * vocabSize) {
      count(a) = Long.MaxValue
      a += 1
    }
    var pos1 = vocabSize - 1
    var pos2 = vocabSize

    var min1i = 0
    var min2i = 0

    a = 0
    while (a < vocabSize - 1) {
      if (pos1 >= 0) {
        if (count(pos1) < count(pos2)) {
          min1i = pos1
          pos1 -= 1
        } else {
          min1i = pos2
          pos2 += 1
        }
      } else {
        min1i = pos2
        pos2 += 1
      }
      if (pos1 >= 0) {
        if (count(pos1) < count(pos2)) {
          min2i = pos1
          pos1 -= 1
        } else {
          min2i = pos2
          pos2 += 1
        }
      } else {
        min2i = pos2
        pos2 += 1
      }
      assert(count(min1i) < Long.MaxValue)
      assert(count(min2i) < Long.MaxValue)
      count(vocabSize + a) = count(min1i) + count(min2i)
      parentNode(min1i) = vocabSize + a
      parentNode(min2i) = vocabSize + a
      binary(min2i) = 1
      a += 1
    }
    // Now assign binary code to each vocabulary word
    var i = 0
    a = 0
    while (a < vocabSize) {
      var b = a
      i = 0
      while (b != vocabSize * 2 - 2) {
        code(i) = binary(b)
        point(i) = b
        i += 1
        b = parentNode(b)
      }
      vocab(a).codeLen = i
      vocab(a).point(0) = vocabSize - 2
      b = 0
      while (b < i) {
        vocab(a).code(i - b - 1) = code(b)
        vocab(a).point(i - b) = point(b) - vocabSize
        b += 1
      }
      a += 1
    }
  }

  /**
   * Computes the vector representation of each word in vocabulary.
   * @param dataset an RDD of sentences,
   *                each sentence is expressed as an iterable collection of words
   * @return a Word2VecOptiModel
   */
  @Since("1.1.0")
  def fit[S <: Iterable[String]](dataset: RDD[S]): Word2VecOptiModel = {


    learnVocab(dataset)

    createBinaryTree()

    val sc = dataset.context

    val expTable = sc.broadcast(createExpTable())
    val bcVocab = sc.broadcast(vocab)
    val bcVocabHash = sc.broadcast(vocabHash)

    try {
      doFit(dataset, sc, expTable, bcVocab, bcVocabHash)
    } finally {
      expTable.destroy(blocking = false)
      bcVocab.destroy(blocking = false)
      //      bcVocabHash.destroy(blocking = false)//å·²ç»é‡Šæ”¾äº†å°±ä¸è¦å†é‡Šæ”¾äº†
    }
  }

  private def doFit[S <: Iterable[String]](
                                            dataset: RDD[S], sc: SparkContext,
                                            expTable: Broadcast[Array[Float]],
                                            bcVocab: Broadcast[Array[VocabWord]],
                                            bcVocabHash: Broadcast[mutable.HashMap[String, Int]]) = {
    println(s"----æ€»è¯çš„ä¸ªæ•°----- $vocabSize")
    // åˆå§‹åŒ–è¾“å…¥å‚æ•°å’Œæ¨¡å‹å‚æ•°
    // åŸå§‹ä»£ç å¦‚ä¸‹ï¼š
    val initRandom = new XORShiftRandom(seed)
    val syn0Global =Array.fill[Float](vocabSize * vectorSize)((initRandom.nextFloat() - 0.5f) / vectorSize)
    val syn1Global = new Array[Float](vocabSize * vectorSize)
    // each partition is a collection of sentences,
    // will be translated into arrays of Index integer
    // å°†è¯RDDå˜æˆå…¶ç´¢å¼•æ„æˆçš„RDD
    val sentences: RDD[Array[Int]] = dataset.mapPartitions { sentenceIter =>
      // Each sentence will map to 0 or more Array[Int]
      sentenceIter.flatMap { sentence =>
        //æ­¤å¤„ä»£ç çš„ç›®çš„æ˜¯ä¸ºäº†å°†å•è¯å¯¹åº”åˆ°ç´¢å¼•ä¸Šï¼Œå˜æˆç´¢å¼•çŸ©é˜µï¼Œå…¶ä¸­ä¸åœ¨è¯å…¸ä¸­çš„ï¼Œä½†æ˜¯å°¾éƒ¨è¯ä¸º-1çš„è¦è¿›è¡Œä¿ç•™ï¼Œå¹¶ç»™ç´¢å¼•-1ï¼Œåç»­ä¸ºäº†æ–¹ä¾¿è¡¨ç¤º
        // Sentence of words, some of which map to a word index
        //val wordIndexes: Iterable[Int] = sentence.flatMap(bcVocabHash.value.get)
        val bcHash = bcVocabHash.value
        val wordIndexes = new ArrayBuffer[Int]()
        for(curStr<-sentence){
          if(bcHash.contains(curStr)){
            wordIndexes.append(bcHash.getOrElse(curStr,-3))
          }
          else if(curStr==EOC){
            wordIndexes.append(-1)
          }
        }
        // break wordIndexes into trunks of maxSentenceLength when has more
        wordIndexes.grouped(maxSentenceLength).map(_.toArray)
      }
    }

    val newSentences: RDD[Array[Int]] = sentences.repartition(numPartitions).cache()

    // æ€»è¯*è¯çš„DIMè‹¥æ¯”æœ€å¤§çš„æ•´æ•°è¿˜å¤§ï¼Œè¯´æ˜ä¸æ”¯æŒè®¡ç®—äº†ï¼Œå»ºè®®è°ƒèŠ‚mincountçš„å€¼ï¼Œä¸¢å¼ƒéƒ¨åˆ†ä½é¢‘è¯
    if (vocabSize.toLong * vectorSize >= Int.MaxValue) {
      throw new RuntimeException("Please increase minCount or decrease vectorSize in Word2VecOpti" +
        " to avoid an OOM. You are highly recommended to make your vocabSize*vectorSize, " +
        "which is " + vocabSize + "*" + vectorSize + " for now, less than `Int.MaxValue`.")
    }


    // æ€»è¯ä¸ªæ•°ï¼Œç­‰äºè¿­ä»£æ¬¡æ•°*trainWordsCount+1  ä¸æ˜ç™½ä¸ºä»€ä¹ˆè¦+1 ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
    val totalWordsCounts = numIterations * trainWordsCount + 1
    var alpha = learningRate

    for (k <- 1 to numIterations) {
      println(s"------è¿­ä»£ç¬¬${k}æ­¥çš„ç»“æœ----------")
      val bcSyn0Global = sc.broadcast(syn0Global)//å¯¹åº” ğ‘¥ğ‘¤ï¼Œæ˜¯è¾“å…¥å‘é‡
      val bcSyn1Global = sc.broadcast(syn1Global)//å¯¹åº” ğœƒğ‘–ğ‘—âˆ’1 ï¼Œæ˜¯å‚æ•°å‘é‡
      val numWordsProcessedInPreviousIterations = (k - 1) * trainWordsCount

      val partial = newSentences.mapPartitionsWithIndex { case (idx, iter) =>
      val random = new XORShiftRandom(seed ^ ((idx + 1) << 16) ^ ((-k - 1) << 8))
      val syn0Modify = new Array[Int](vocabSize)
      val syn1Modify = new Array[Int](vocabSize)
      val model: (Array[Float], Array[Float], Long, Long) = iter.foldLeft((bcSyn0Global.value, bcSyn1Global.value, 0L, 0L)) {
        case ((syn0, syn1, lastWordCount, wordCount), sentence) =>
          var lwc = lastWordCount
          var wc = wordCount
          if (wordCount - lastWordCount > 10000) {
            lwc = wordCount
            //å­¦ä¹ ç‡å˜åŒ–çš„è¿‡ç¨‹
            alpha = learningRate *
              (1 - (numPartitions * wordCount.toDouble + numWordsProcessedInPreviousIterations) /
                totalWordsCounts)
            if (alpha < learningRate * 0.0001) alpha = learningRate * 0.0001
            logInfo(s"wordCount = ${wordCount + numWordsProcessedInPreviousIterations}, " +
              s"alpha = $alpha")
          }
          // è‹¥æœ€ç»ˆçš„è¯ä¸ºEOCï¼Œåˆ™å°†å½“å‰è¯çš„é•¿åº¦å‡1
          var curLength = sentence.length
          val lastIndex: Int = sentence(curLength-1)
          if(lastIndex == -1){
            curLength -=1
            println("\n\n===========å°¾éƒ¨æ˜¯-1ï¼Œè¡¨ç¤ºå€’æ•°ç¬¬äºŒä¸ªè¯è¦ä½œä¸ºçª—å£è¯è¿›è¡Œæ›´æ–°===========")
          }
          else {
            println("\n\n===================å°¾éƒ¨ä¸æ˜¯-1===========")
          }
          wc += curLength
          // æœ€åä¸€ä¸ªè¯-1ä¸ä¼šä½œä¸ºä¸­å¿ƒè¯è¿›è¡Œæ›´æ–°
        for(pos<- 0 until curLength if !(lastIndex == -1 && pos == curLength - 1)) {
          //æ­¤wordæ˜¯ä¸­å¿ƒè¯
          val word: Int = sentence(pos)
          // çª—å£è¯æ›´æ–°çš„è¿‡ç¨‹ä¸­å¹¶ä¸æ˜¯çª—å£å†…æ‰€æœ‰è¯éƒ½æ›´æ–°ï¼Œåªéšæœºæ›´æ–°è®¾å®šçª—å£å¤§å°å†…çš„éšæœºçš„ä¸€ä¸ªçª—å£ã€‚æ¯”å¦‚window=4, b=2,åˆ™åªæ›´æ–°-2ï¼Œ-1ï¼Œ1ï¼Œ2ã€‚b=3ï¼Œåˆ™åªæ›´æ–°-1ï¼Œ1
          val b = random.nextInt(window)
          print(s"\n----ä¸­å¿ƒè¯ä¸º:${bcVocab.value(word).word},éšæœºæ•°ä¸ºï¼šb=$b åˆ™$pos çš„çª—å£å¤§å°ä¸º${window-b}çš„è¯ä¸‹æ ‡ä¸ºï¼š")
          // Train Skip-gram
          // TODO å®šä¹‰ä¸€ä¸ªéå†çš„çª—å£è¯çš„æ•°ç»„ï¼Œè‹¥å°¾éƒ¨è¯æ˜¯-1ï¼Œè¯´æ˜å€’æ•°ç¬¬äºŒä¸ªè¯æ˜¯ä¸‹å•è¯ï¼Œåˆ™å€’æ•°ç¬¬äºŒä¸ªè¯è¦è¢«å½“åšçª—å£è¯è¿›è¡Œæ›´æ–°
          val tmp1 = (b until window * 2 + 1 - b).toArray
          val addInt = curLength-1-pos+window
          val contenxtIndexArr: Seq[Int] = if(lastIndex == -1 && !tmp1.contains(addInt)){
            tmp1:+addInt
          }else{
            tmp1
          }
          // å‚æ•°æ›´æ–°è¿‡ç¨‹è§https://www.cnblogs.com/pinard/p/7243513.html
          // å®é™…ä¸Šæ˜¯ä»¥bä¸ºä¸­å¿ƒï¼Œæ›´æ–°å…¶å·¦å³çª—å£å¤§å°ä¸ºwindow-bçš„æ‰€æœ‰è¯ã€‚ä¾‹å¦‚çª—å£å¤§å°ç­‰äº5ï¼Œb=2ï¼Œå®é™…ä¸Šåªæ›´æ–°ä¸­å¿ƒè¯ä¸Šä¸‹æ–‡4ä¸ªè¯
          for(a <- contenxtIndexArr){
            if (a != window) {
              val c = pos - window + a
              //è¯´æ˜ä¸Šä¸‹æ–‡ä¸­çš„è¯ï¼Œè¿˜åœ¨sentenceå†…éƒ¨
              if (c >= 0 && c < curLength) {
                print(s"$c ,")
                val lastWord: Int = sentence(c) //ä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡è¯çš„ç´¢å¼•
                val l1 = lastWord * vectorSize // åœ¨å‚æ•°å‘é‡çš„èµ·å§‹ä½ç½®
                val neu1e = new Array[Float](vectorSize) // å¯¹åº”eå‘é‡ã€‚
                // Hierarchical softmax
                var d = 0
                // æ›´æ–°HUFFMANå†…éƒ¨èŠ‚ç‚¹çš„å‚æ•°
                while (d < bcVocab.value(word).codeLen) {
                  val inner: Int = bcVocab.value(word).point(d) //è·¯å¾„ä¸Šçš„èŠ‚ç‚¹index
                  // è¾“å…¥å‘é‡çš„èµ·å§‹ä½ç½®
                  val l2 = inner * vectorSize
                  // Propagate hidden -> output
                  //å‘é‡ç‚¹ä¹˜ï¼Œsyn0 .* syn1
                  var f = blas.sdot(vectorSize, syn0, l1, 1, syn1, l2, 1)
                  if (f > -MAX_EXP && f < MAX_EXP) {
                    val ind = ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2.0)).toInt
                    f = expTable.value(ind)
                    val g = ((1 - bcVocab.value(word).code(d) - f) * alpha).toFloat
                    //neu1e = g * syn1 + neu1e, å€’æ•°ç¬¬äºŒä¸ªå‚æ•°ä¸º0çš„åŸå› æ˜¯eå‘é‡ä½œä¸ºä¸´æ—¶å‘é‡è¿›è¡Œè®¡ç®—ï¼Œå…¶å¤§å°å°±åªæœ‰vectorSize
                    blas.saxpy(vectorSize, g, syn1, l2, 1, neu1e, 0, 1)
                    //syn1 = g * syn0 + syn1
                    blas.saxpy(vectorSize, g, syn0, l1, 1, syn1, l2, 1)
                    syn1Modify(inner) += 1
                  }
                  d += 1
                  // syn0 = syn0+neu1e
                  blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, l1, 1)
                  syn0Modify(lastWord) += 1 //ä¿å­˜syn0çš„è¿­ä»£å˜åŒ–æ¬¡æ•°
                }
              }
            }
          }
        }
          (syn0, syn1, lwc, wc)
        }
        val syn0Local = model._1
        val syn1Local = model._2
        // Only output modified vectors.
        Iterator.tabulate(vocabSize) { index =>
          if (syn0Modify(index) > 0) {
            Some((index, syn0Local.slice(index * vectorSize, (index + 1) * vectorSize)))
          } else {
            None
          }
        }.flatten ++ Iterator.tabulate(vocabSize) { index =>
          if (syn1Modify(index) > 0) {
            Some((index + vocabSize, syn1Local.slice(index * vectorSize, (index + 1) * vectorSize)))
          } else {
            None
          }
        }.flatten
      }
      // SPARK-24666: do normalization for aggregating weights from partitions.
      // Original Word2VecOpti either single-thread or multi-thread which do Hogwild-style aggregation.
      // Our approach needs to do extra normalization, otherwise adding weights continuously may
      // cause overflow on float and lead to infinity/-infinity weights.
      val synAgg = partial.mapPartitions { iter =>
        iter.map { case (id, vec) =>
          (id, (vec, 1))
        }
      }.reduceByKey { case ((v1, count1), (v2, count2)) =>
        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)
        (v1, count1 + count2)
      }.map { case (id, (vec, count)) =>
        blas.sscal(vectorSize, 1.0f / count, vec, 1)
        (id, vec)
      }.collect()
      var i = 0
      while (i < synAgg.length) {
        val index = synAgg(i)._1
        if (index < vocabSize) {
          Array.copy(synAgg(i)._2, 0, syn0Global, index * vectorSize, vectorSize)
        } else {
          Array.copy(synAgg(i)._2, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)
        }
        i += 1
      }
      bcSyn0Global.destroy(false)
      bcSyn1Global.destroy(false)
    }
    newSentences.unpersist()

    val wordArray = vocab.map(_.word)
    new Word2VecOptiModel(wordArray.zipWithIndex.toMap, syn0Global)
  }

  /**
   * Computes the vector representation of each word in vocabulary (Java version).
   * @param dataset a JavaRDD of words
   * @return a Word2VecOptiModel
   */
  @Since("1.1.0")
  def fit[S <: JavaIterable[String]](dataset: JavaRDD[S]): Word2VecOptiModel = {
    fit(dataset.rdd.map(_.asScala))
  }
}

/**
 * Word2VecOpti model
 * @param wordIndex maps each word to an index, which can retrieve the corresponding
 *                  vector from wordVectors
 * @param wordVectors array of length numWords * vectorSize, vector corresponding
 *                    to the word mapped with index i can be retrieved by the slice
 *                    (i * vectorSize, i * vectorSize + vectorSize)
 */
@Since("1.1.0")
class Word2VecOptiModel private[spark] (
                                         private[spark] val wordIndex: Map[String, Int],
                                         private[spark] val wordVectors: Array[Float]) extends Serializable with Saveable {

  private val numWords = wordIndex.size
  // vectorSize: Dimension of each word's vector.
  private val vectorSize = wordVectors.length / numWords

  // wordList: Ordered list of words obtained from wordIndex.
  private val wordList: Array[String] = {
    val (wl, _) = wordIndex.toSeq.sortBy(_._2).unzip
    wl.toArray
  }

  // wordVecNorms: Array of length numWords, each value being the Euclidean norm
  //               of the wordVector.
  private val wordVecNorms: Array[Float] = {
    val wordVecNorms = new Array[Float](numWords)
    var i = 0
    while (i < numWords) {
      val vec = wordVectors.slice(i * vectorSize, i * vectorSize + vectorSize)
      wordVecNorms(i) = blas.snrm2(vectorSize, vec, 1)
      i += 1
    }
    wordVecNorms
  }

  @Since("1.5.0")
  def this(model: Map[String, Array[Float]]) = {
    this(Word2VecOptiModel.buildWordIndex(model), Word2VecOptiModel.buildWordVectors(model))
  }

  override protected def formatVersion = "1.0"

  @Since("1.4.0")
  def save(sc: SparkContext, path: String): Unit = {
    Word2VecOptiModel.SaveLoadV1_0.save(sc, path, getVectors)
  }

  /**
   * Transforms a word to its vector representation
   * @param word a word
   * @return vector representation of word
   */
  @Since("1.1.0")
  def transform(word: String): Vector = {
    wordIndex.get(word) match {
      case Some(ind) =>
        val vec = wordVectors.slice(ind * vectorSize, ind * vectorSize + vectorSize)
        Vectors.dense(vec.map(_.toDouble))
      case None =>
        throw new IllegalStateException(s"$word not in vocabulary")
    }
  }

  /**
   * Find synonyms of a word; do not include the word itself in results.
   * @param word a word
   * @param num number of synonyms to find
   * @return array of (word, cosineSimilarity)
   */
  @Since("1.1.0")
  def findSynonyms(word: String, num: Int): Array[(String, Double)] = {
    val vector = transform(word)
    findSynonyms(vector, num, Some(word))
  }

  /**
   * Find synonyms of the vector representation of a word, possibly
   * including any words in the model vocabulary whose vector respresentation
   * is the supplied vector.
   * @param vector vector representation of a word
   * @param num number of synonyms to find
   * @return array of (word, cosineSimilarity)
   */
  @Since("1.1.0")
  def findSynonyms(vector: Vector, num: Int): Array[(String, Double)] = {
    findSynonyms(vector, num, None)
  }

  /**
   * Find synonyms of the vector representation of a word, rejecting
   * words identical to the value of wordOpt, if one is supplied.
   * @param vector vector representation of a word
   * @param num number of synonyms to find
   * @param wordOpt optionally, a word to reject from the results list
   * @return array of (word, cosineSimilarity)
   */
  private def findSynonyms(
                            vector: Vector,
                            num: Int,
                            wordOpt: Option[String]): Array[(String, Double)] = {
    require(num > 0, "Number of similar words should > 0")

    val fVector = vector.toArray.map(_.toFloat)
    val cosineVec = new Array[Float](numWords)
    val alpha: Float = 1
    val beta: Float = 0
    // Normalize input vector before blas.sgemv to avoid Inf value
    val vecNorm = blas.snrm2(vectorSize, fVector, 1)
    if (vecNorm != 0.0f) {
      blas.sscal(vectorSize, 1 / vecNorm, fVector, 0, 1)
    }
    blas.sgemv(
      "T", vectorSize, numWords, alpha, wordVectors, vectorSize, fVector, 1, beta, cosineVec, 1)

    var i = 0
    while (i < numWords) {
      val norm = wordVecNorms(i)
      if (norm == 0.0f) {
        cosineVec(i) = 0.0f
      } else {
        cosineVec(i) /= norm
      }
      i += 1
    }

    val pq = new BoundedPriorityQueue[(String, Float)](num + 1)(Ordering.by(_._2))

    var j = 0
    while (j < numWords) {
      pq += Tuple2(wordList(j), cosineVec(j))
      j += 1
    }

    val scored = pq.toSeq.sortBy(-_._2)

    val filtered = wordOpt match {
      case Some(w) => scored.filter(tup => w != tup._1)
      case None => scored
    }

    filtered
      .take(num)
      .map { case (word, score) => (word, score.toDouble) }
      .toArray
  }

  /**
   * Returns a map of words to their vector representations.
   */
  @Since("1.2.0")
  def getVectors: Map[String, Array[Float]] = {
    wordIndex.map { case (word, ind) =>
      (word, wordVectors.slice(vectorSize * ind, vectorSize * ind + vectorSize))
    }
  }

}

@Since("1.4.0")
object Word2VecOptiModel extends Loader[Word2VecOptiModel] {

  private def buildWordIndex(model: Map[String, Array[Float]]): Map[String, Int] = {
    model.keys.zipWithIndex.toMap
  }

  private def buildWordVectors(model: Map[String, Array[Float]]): Array[Float] = {
    require(model.nonEmpty, "Word2VecOptiMap should be non-empty")
    val (vectorSize, numWords) = (model.head._2.length, model.size)
    val wordList = model.keys.toArray
    val wordVectors = new Array[Float](vectorSize * numWords)
    var i = 0
    while (i < numWords) {
      Array.copy(model(wordList(i)), 0, wordVectors, i * vectorSize, vectorSize)
      i += 1
    }
    wordVectors
  }

  private object SaveLoadV1_0 {

    val formatVersionV1_0 = "1.0"

    val classNameV1_0 = "org.apache.spark.mllib.feature.Word2VecOptiModel"

    case class Data(word: String, vector: Array[Float])

    def load(sc: SparkContext, path: String): Word2VecOptiModel = {
      val spark = SparkSession.builder().sparkContext(sc).getOrCreate()
      val dataFrame = spark.read.parquet(Loader.dataPath(path))
      // Check schema explicitly since erasure makes it hard to use match-case for checking.
      Loader.checkSchema[Data](dataFrame.schema)

      val dataArray = dataFrame.select("word", "vector").collect()
      val Word2VecOptiMap = dataArray.map(i => (i.getString(0), i.getSeq[Float](1).toArray)).toMap
      new Word2VecOptiModel(Word2VecOptiMap)
    }

    def save(sc: SparkContext, path: String, model: Map[String, Array[Float]]): Unit = {
      val spark = SparkSession.builder().sparkContext(sc).getOrCreate()

      val vectorSize = model.values.head.length
      val numWords = model.size
      val metadata = compact(render(
        ("class" -> classNameV1_0) ~ ("version" -> formatVersionV1_0) ~
          ("vectorSize" -> vectorSize) ~ ("numWords" -> numWords)))
      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))

      // We want to partition the model in partitions smaller than
      // spark.kryoserializer.buffer.max
      val bufferSize = Utils.byteStringAsBytes(
        spark.conf.get("spark.kryoserializer.buffer.max", "64m"))
      // We calculate the approximate size of the model
      // We only calculate the array size, considering an
      // average string size of 15 bytes, the formula is:
      // (floatSize * vectorSize + 15) * numWords
      val approxSize = (4L * vectorSize + 15) * numWords
      val nPartitions = ((approxSize / bufferSize) + 1).toInt
      val dataArray = model.toSeq.map { case (w, v) => Data(w, v) }
      spark.createDataFrame(dataArray).repartition(nPartitions).write.parquet(Loader.dataPath(path))
    }
  }

  @Since("1.4.0")
  override def load(sc: SparkContext, path: String): Word2VecOptiModel = {

    val (loadedClassName, loadedVersion, metadata) = Loader.loadMetadata(sc, path)
    implicit val formats = DefaultFormats
    val expectedVectorSize = (metadata \ "vectorSize").extract[Int]
    val expectedNumWords = (metadata \ "numWords").extract[Int]
    val classNameV1_0 = SaveLoadV1_0.classNameV1_0
    (loadedClassName, loadedVersion) match {
      case (classNameV1_0, "1.0") =>
        val model = SaveLoadV1_0.load(sc, path)
        val vectorSize = model.getVectors.values.head.length
        val numWords = model.getVectors.size
        require(expectedVectorSize == vectorSize,
          s"Word2VecOptiModel requires each word to be mapped to a vector of size " +
            s"$expectedVectorSize, got vector of size $vectorSize")
        require(expectedNumWords == numWords,
          s"Word2VecOptiModel requires $expectedNumWords words, but got $numWords")
        model
      case _ => throw new Exception(
        s"Word2VecOptiModel.load did not recognize model with (className, format version):" +
          s"($loadedClassName, $loadedVersion).  Supported:\n" +
          s"  ($classNameV1_0, 1.0)")
    }
  }
}
